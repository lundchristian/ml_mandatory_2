{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the standard imports\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2A\n",
    "\n",
    "Data is appended as `data_problem2.csv`.\n",
    "\n",
    "Load the data and report general information of the data.\n",
    "\n",
    "Additionally plot (as histograms) the data and discuss the separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the headers and transpose the data\n",
    "# this gives us 3600 samples and 2 features\n",
    "# col 1 is a class label (0 or 1), and col 0 is the actual feature (some value)\n",
    "data = pd.read_csv(\"data_problem2.csv\", header=None)\n",
    "data = data.T\n",
    "# call on the methods below to get a better understanding of the data\n",
    "# data.head()\n",
    "# data.info()\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data.iloc[:, 1]\n",
    "feature = data.iloc[:, 0]\n",
    "# count occurrences of class 0 and 1\n",
    "class_0_count = (data == 0).sum().sum()\n",
    "class_1_count = (data == 1).sum().sum()\n",
    "print(f\"Class 0 count:          {class_0_count}\")\n",
    "print(f\"Class 1 count:          {class_1_count}\")\n",
    "print(f\"Label missing values:   {label.isnull().sum()}\")\n",
    "print(f\"Feature missing values: {feature.isnull().sum()}\")\n",
    "# plot the histogram and scatter plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 12))\n",
    "axs[0].bar([\"Class 0\", \"Class 1\"], [class_0_count, class_1_count], color=[\"blue\", \"green\"])\n",
    "axs[0].set_title(\"Histogram of classes 0 and 1 for all samples\")\n",
    "axs[0].set_xlabel(\"Class\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "axs[1].scatter(label, feature, color=\"black\")\n",
    "axs[1].set_title(\"Scatter plot\")\n",
    "axs[1].set_xlabel(\"Class 0 or 1\")\n",
    "axs[1].set_ylabel(\"Feature value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2C\n",
    "\n",
    "Split the data into training and test data.\n",
    "\n",
    "Use the maximum likelihood estimations to estimate the parameters based on the training data.\n",
    "\n",
    "Use the point-estimations of the parameters to implement a Bayes’ classifier.\n",
    "\n",
    "Report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the 80/20 split\n",
    "class_0 = data[data.iloc[:, 1] == 0]\n",
    "class_1 = data[data.iloc[:, 1] == 1]\n",
    "# extract pseudo-randomly 80% to use for training\n",
    "training_class_0 = class_0.sample(frac=0.8, random_state=random.randint(1, 100))\n",
    "training_class_1 = class_1.sample(frac=0.8, random_state=random.randint(1, 100))\n",
    "# extract pseudo-randomly 20% to use for testing\n",
    "testing_class_0 = class_0.drop(training_class_0.index)\n",
    "testing_class_1 = class_1.drop(training_class_1.index)\n",
    "# merge the sets back together, and reset the sample index\n",
    "training_set = pd.concat([training_class_0, training_class_1]).reset_index(drop=True)\n",
    "testing_set = pd.concat([testing_class_0, testing_class_1]).reset_index(drop=True)\n",
    "# list the stats\n",
    "print(f\"Number of samples in data set:        {data.shape[0]}\")\n",
    "print(f\"80% of data set:                      {int(data.shape[0] * 0.8)}\")\n",
    "print(f\"20% of data set:                      {int(data.shape[0] * 0.2)}\")\n",
    "print(f\"Number of samples in training set:    {training_set.shape[0]}\")\n",
    "print(f\"Number of samples in testing set:     {testing_set.shape[0]}\")\n",
    "print(f\"Class 0 percentage of dataset:        {round(data[data.iloc[:, 1] == 0].shape[0] / data.shape[0] * 100, 2)}\")\n",
    "print(f\"Class 1 percentage of dataset:        {round(data[data.iloc[:, 1] == 1].shape[0] / data.shape[0] * 100, 2)}\")\n",
    "print(f\"Class 0 percentage of training set:   {round(training_set[training_set.iloc[:, 1] == 0].shape[0] / training_set.shape[0] * 100, 2)}\")\n",
    "print(f\"Class 1 percentage of training set:   {round(training_set[training_set.iloc[:, 1] == 1].shape[0] / training_set.shape[0] * 100, 2)}\")\n",
    "print(f\"Class 0 percentage of testing set:    {round(testing_set[testing_set.iloc[:, 1] == 0].shape[0] / testing_set.shape[0] * 100, 2)}\")\n",
    "print(f\"Class 1 percentage of testing set:    {round(testing_set[testing_set.iloc[:, 1] == 1].shape[0] / testing_set.shape[0] * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the MLE's for the training set\n",
    "# class 0 (Gamma distribution given alpha=2)\n",
    "alpha = 2\n",
    "train_c0_set = training_set[training_set.iloc[:, 1] == 0]\n",
    "train_c0_feature = train_c0_set.iloc[:, 0]\n",
    "n0 = train_c0_set.shape[0]\n",
    "# class 1 (Gaussian distribution)\n",
    "train_c1_set = training_set[training_set.iloc[:, 1] == 1]\n",
    "train_c1_feature = train_c1_set.iloc[:, 0]\n",
    "n1 = train_c1_set.shape[0]\n",
    "# param MLE's\n",
    "beta_hat_c0 = np.sum(train_c0_feature) / (n0 * alpha)\n",
    "mu_hat_c1 = np.sum(train_c1_feature) / n1\n",
    "sigma2_hat_c1 = np.sum((train_c1_feature - mu_hat_c1) ** 2) / n1\n",
    "# fetch the testing set n's\n",
    "n0_test = testing_set[testing_set.iloc[:, 1] == 0].shape[0]\n",
    "n1_test = testing_set[testing_set.iloc[:, 1] == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the point-estimations of the parameters to implement a Bayes’ classifier\n",
    "def bayes_classifier(x, alpha, beta_hat_0, mu_hat_1, sigma2_hat_1, p_c0, p_c1):\n",
    "    sigma_hat_1 = np.sqrt(sigma2_hat_1)\n",
    "    # p(x|C0) - Gamma distribution (likelihood)\n",
    "    p_x_c0 = (1 / (beta_hat_0 ** alpha)) * (x ** (alpha - 1)) * np.exp(-x / beta_hat_0)\n",
    "    # p(x|C1) - Gaussian distribution (likelihood)\n",
    "    p_x_c1 = (1 / (sigma_hat_1 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu_hat_1) / sigma_hat_1) ** 2)\n",
    "    # (evidence)\n",
    "    p_x = p_x_c0 * p_c0 + p_x_c1 * p_c1\n",
    "    # (posteriors)\n",
    "    p_c0_x = (p_x_c0 * p_c0) / p_x\n",
    "    p_c1_x = (p_x_c1 * p_c1) / p_x\n",
    "    # classify\n",
    "    return 0 if p_c0_x > p_c1_x else 1\n",
    "\n",
    "training_labels = training_set.iloc[:, 1]\n",
    "training_features = training_set.iloc[:, 0]\n",
    "# (priors)\n",
    "p_c0_train = n0 / (n0 + n1)\n",
    "p_c1_train = n1 / (n0 + n1)\n",
    "# functional style, map the bayes classifier to each sample\n",
    "predictions_on_train = training_features.map(lambda x: bayes_classifier(x, alpha, beta_hat_c0, mu_hat_c1, sigma2_hat_c1, p_c0_train, p_c1_train))\n",
    "\n",
    "testing_labels = testing_set.iloc[:, 1]\n",
    "testing_features = testing_set.iloc[:, 0]\n",
    "# (priors)\n",
    "p_c0_test = n0_test / (n0_test + n1_test)\n",
    "p_c1_test = n1_test / (n0_test + n1_test)\n",
    "# functional style, map the bayes classifier to each sample\n",
    "predictions_on_test = testing_features.map(lambda x: bayes_classifier(x, alpha, beta_hat_c0, mu_hat_c1, sigma2_hat_c1, p_c0_test, p_c1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and report the accuracy of the classifier on the training set\n",
    "def evaluate_classification(predictions, labels):\n",
    "    correct_predictions = predictions == labels # element-wise comparison\n",
    "    accuracy = np.mean(correct_predictions) * 100\n",
    "    tp = ((predictions == 1) & (labels == 1)).sum()\n",
    "    tn = ((predictions == 0) & (labels == 0)).sum()\n",
    "    fp = ((predictions == 1) & (labels == 0)).sum()\n",
    "    fn = ((predictions == 0) & (labels == 1)).sum()\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"Accuracy:          {accuracy:.2f}%\")\n",
    "    print(f\"Precision:         {precision:.2f}\")\n",
    "    print(f\"Recall:            {recall:.2f}\")\n",
    "    print(f\"F1 Score:          {f1_score:.2f}\")\n",
    "    print(f\"Correctly classified Points: {(correct_predictions).sum()}\")\n",
    "    print(f\"Misclassified Points:        {(~correct_predictions).sum()}\")\n",
    "\n",
    "evaluate_classification(predictions_on_train, training_labels)\n",
    "evaluate_classification(predictions_on_test, testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2D\n",
    "\n",
    "Explain why the Bayes’ classifier minimizes the probability of miss-classification when the probability distribution of the data is known.\n",
    "\n",
    "Show the misclassified data in a plot along with the rest of the data and explain why it was misclassified.\n",
    "\n",
    "Does it follow the conclusion in task 2A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classification(predictions, labels, features):\n",
    "    correct = predictions == labels # element-wise comparison\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    # correctly classified points\n",
    "    plt.scatter(labels[correct], features[correct], color=\"green\", label=\"Correctly classified\")\n",
    "    # misclassified points\n",
    "    plt.scatter(labels[~correct], features[~correct], color=\"red\", label=\"Misclassified\")\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"Classification results\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_classification(predictions_on_train, training_labels, training_features)\n",
    "visualize_classification(predictions_on_test, testing_labels, testing_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
